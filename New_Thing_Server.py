# -*- coding: utf-8 -*-
"""federated_learning_for_DRED_(1)_(2)_(3)_(1)_(1)_(1)_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ct1HPPlafpPj6B94l0x0aq9Zqp1ICpF9

# Federated Learning for DRED
"""
#!pip install tensorflow_federated

#import os
#os.kill(os.getpid(), 9) #For reconnecting runtime

#@test {"skip": true}

#!pip install --quiet --upgrade tensorflow-federated

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
#print("Hello")

import collections
import pandas as pd

import numpy as np

# Print the first few rows of the DataFrame
import tensorflow as tf
import tensorflow_federated as tff
#from sklearn.model_selection import train_test_split
from collections.abc import Callable
np.random.seed(0)
import greet_server
#tff.federated_computation(lambda: 'Hello, World!')()
#print("Hello")
# Load the dataset while specifying data types for each column

"""### Preparing the input data

Federated learning requires a federated data set,
i.e., a collection of data from multiple users. Federated data is typically
non-[i.i.d.]
which poses a unique set of challenges.



Here's how we can load it:
"""


# Load the dataset while specifying data types for each column
file_path = 'new_dataset.csv'

dtypes = {'Client': int, 'Feature1': float, 'Feature2': float, 'Feature3': float, 'Feature4': float, 'Feature5': float, 'Label': int}
num_rows = 166353
nrows = int(num_rows/2)
df = pd.read_csv(file_path, header=0, dtype=dtypes, nrows=nrows)

# Print the first few rows of the DataFrame
print(df.head())

#print("Shape of each column:")
for column in df.columns:
    shape = df[column].shape
    #print(f"{column}: {shape[0]}")


#column 2 for our purpose chosen
client_id_colname = 'Client' # the column that represents client ID
SHUFFLE_BUFFER = 1000
NUM_EPOCHS = 1
# split client id into train and test clients
df[client_id_colname] = df[client_id_colname].astype(int)
client_ids = df[client_id_colname].unique()

#train_client_ids = client_ids.sample(frac=0.5).tolist()

# Convert client_ids to a Pandas Series
client_ids_series = pd.Series(client_ids)
# Sample 70% of the client IDs for training
train_client_ids = client_ids_series.sample(frac=0.7).tolist()
test_client_ids = [x for x in client_ids if x not in train_client_ids]

def serializable_dataset_fn(client_id):
  # a function which takes a client_id and returns a tf.data.Dataset for that client
  client_data = df[df[client_id_colname] == client_id]
  dataset = tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))
  dataset = dataset.shuffle(SHUFFLE_BUFFER).batch(1).repeat(NUM_EPOCHS)
  return dataset

# from_clients_and_fn  to  from_clients_and_tf_fn
# Substitute  create_tf_dataset_for_client_fn  with  serializable_dataset_fn parameter.
train_data = tff.simulation.datasets.TestClientData.from_clients_and_tf_fn(

        client_ids=train_client_ids,
        serializable_dataset_fn=serializable_dataset_fn
    )
test_data = tff.simulation.datasets.TestClientData.from_clients_and_tf_fn(
        client_ids=test_client_ids,
        serializable_dataset_fn=serializable_dataset_fn
    )

train_data.element_type_structure

print(df.columns)

example_dataset = train_data.serializable_dataset_fn(
        train_data.client_ids[0]
    )
print(type(example_dataset))
example_element = iter(example_dataset).__next__()
print(example_element)

example_dataset = train_data.create_tf_dataset_for_client(
    train_data.client_ids[23])

example_element = next(iter(example_dataset))
example_element['Feature2'].numpy()[0]

"""### Preprocessing the input data"""

NUM_CLIENTS = 10
# We can change it. It is not matter now.
NUM_EPOCHS = 5
BATCH_SIZE = 20
SHUFFLE_BUFFER = 100
PREFETCH_BUFFER = 10

def preprocess(dataset):

    def batch_format_fn(element):
        """Flatten a batch `pixels` and return the features as an `OrderedDict`."""
        # Concatenate and reshape all features into a 2D array
        x = tf.concat([
            tf.reshape(element['Feature1'], [-1, 1]),
            tf.reshape(element['Feature2'], [-1, 1]),
            tf.reshape(element['Feature3'], [-1, 1]),
            tf.reshape(element['Feature4'], [-1, 1]),
            tf.reshape(element['Feature5'], [-1, 1])
        ], axis=1)

        return collections.OrderedDict(
            x=x,
            y=tf.reshape(element['Label'], [-1, 1])
        )

    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).batch(
        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)

print(example_dataset)

preprocessed_example_dataset = preprocess(example_dataset)

sample_batch = tf.nest.map_structure(lambda x: x.numpy(),
                                     next(iter(preprocessed_example_dataset)))

sample_batch

def make_federated_data(client_data, client_ids):
  return [
      preprocess(client_data.create_tf_dataset_for_client(x))
      for x in client_ids
  ]

sample_clients = train_data.client_ids[0:NUM_CLIENTS]

federated_train_data = make_federated_data(train_data, sample_clients)

print(f'Number of client datasets: {len(federated_train_data)}')
print(f'First dataset: {federated_train_data[0]}')

"""## Customizing the model implementation



"""

DredVariables = collections.namedtuple(
    'DredVariables', 'weights bias num_examples loss_sum accuracy_sum')

print(DredVariables)
# Call the SayHello function from greet_server.py
# Access global variables
weight = greet_server.global_weights
bias = greet_server.global_biases
def create_dred_variables(weight=None,bias=None):
    if weight is None and bias is None:
        weight_init = lambda: tf.zeros(dtype=tf.float32, shape=(5, 10))
        bias_init = lambda: tf.zeros(dtype=tf.float32, shape=(10))
    else:
        weight_init = lambda: weight
        bias_init = lambda: bias
    return DredVariables(
      weights=tf.Variable(
          weight_init,
          name='weights',
          trainable=True),
      bias=tf.Variable(
          bias_init,
          name='bias',
          trainable=True),
      num_examples=tf.Variable(0.0, name='num_examples', trainable=False),
      loss_sum=tf.Variable(0.0, name='loss_sum', trainable=False),
  accuracy_sum=tf.Variable(0.0, name='accuracy_sum', trainable=False))

"""with the variables for model parameters and cumulative statistics in place, we
can now define the forward pass method that computes loss, emits predictions,
and updates the cumulative statistics for a single batch of input data, as
follows.
"""

def predict_on_batch(Variables, x):
  return tf.nn.softmax(tf.matmul(x, Variables.weights) + Variables.bias)

def dred_forward_pass(Variables, batch):
  y = predict_on_batch(Variables, batch['x'])
  predictions = tf.cast(tf.argmax(y, 1), tf.int32)

  flat_labels = tf.reshape(batch['y'], [-1])
  loss = -tf.reduce_mean(
      tf.reduce_sum(tf.one_hot(flat_labels, 10) * tf.math.log(y), axis=[1]))
  accuracy = tf.reduce_mean(
      tf.cast(tf.equal(predictions, flat_labels), tf.float32))

  num_examples = tf.cast(tf.size(batch['y']), tf.float32)

  Variables.num_examples.assign_add(num_examples)
  Variables.loss_sum.assign_add(loss * num_examples)
  Variables.accuracy_sum.assign_add(accuracy * num_examples)

  return loss, predictions
#print("helloyou")
"""next, we define two functions that are related to local metrics, again using tensorflow.

the first function `get_local_unfinalized_metrics` returns the unfinalized metric values (in addition to model updates, which are handled automatically) that are eligible to be aggregated to the server in a federated learning or evaluation process.
"""

def get_local_unfinalized_metrics(variables):
  return collections.OrderedDict(
      num_examples=[variables.num_examples],
      loss=[variables.loss_sum, variables.num_examples],
      accuracy=[variables.accuracy_sum, variables.num_examples])

"""the second function `get_metric_finalizers` returns an `ordereddict` of `tf.function`s with the same keys (i.e., metric names) as `get_local_unfinalized_metrics`. each `tf.function` takes in the metric's unfinalized values and computes the finalized metric."""

def get_metric_finalizers():
  return collections.OrderedDict(
      num_examples=tf.function(func=lambda x: x[0]),
      loss=tf.function(func=lambda x: x[0] / x[1]),
      accuracy=tf.function(func=lambda x: x[0] / x[1]))

"""how the local unfinalized metrics returned by `get_local_unfinalized_metrics` are aggregated across clients are specified by the `metrics_aggregator` parameter when defining the federated learning or evaluation processes.

### constructing an instance of `tff.learning.models.variablemodel`

with all of the above in place, we are ready to construct a model representation
for use with tff similar to one that's generated for you when you let tff ingest
a keras model.
"""

#print("hellome")

class dredmodel(tff.learning.models.VariableModel):

  def __init__(self):
    self._variables = create_dred_variables()

  @property
  def trainable_variables(self):
    return [self._variables.weights, self._variables.bias]

  @property
  def non_trainable_variables(self):
    return []

  @property
  def local_variables(self):
    return [
        self._variables.num_examples, self._variables.loss_sum,
        self._variables.accuracy_sum
    ]

  @property
  def input_spec(self):
    return collections.OrderedDict(
        x=tf.TensorSpec([None, 5], tf.float32),
        y=tf.TensorSpec([None, 1], tf.int32))

  @tf.function
  def predict_on_batch(self, x, training=True):
    del training
    return predict_on_batch(self._variables, x)

  @tf.function
  def forward_pass(self, batch, training=True):
    del training
    loss, predictions = dred_forward_pass(self._variables, batch)
    num_exmaples = tf.shape(batch['x'])[0]
    return tff.learning.models.BatchOutput(
        loss=loss, predictions=predictions, num_examples=num_exmaples)

  @tf.function
  def report_local_unfinalized_metrics(
      self) -> collections.OrderedDict[str, list[tf.Tensor]]:
    """creates an `ordereddict` of metric names to unfinalized values."""
    return get_local_unfinalized_metrics(self._variables)

  def metric_finalizers(
      self) -> collections.OrderedDict[str, Callable[[list[tf.Tensor]], tf.Tensor]]:
    """creates an `ordereddict` of metric names to finalizers."""
    return get_metric_finalizers()

  @tf.function
  def reset_metrics(self):
    """resets metrics variables to initial value."""
    for var in self.local_variables:
      var.assign(tf.zeros_like(var))

class dredmodel2(tff.learning.models.VariableModel):

  def __init__(self):
    global in_weight,in_bias
    self._variables = create_dred_variables(in_weight,in_bias)

  @property
  def trainable_variables(self):
    return [self._variables.weights, self._variables.bias]

  @property
  def non_trainable_variables(self):
    return []

  @property
  def local_variables(self):
    return [
        self._variables.num_examples, self._variables.loss_sum,
        self._variables.accuracy_sum
    ]

  @property
  def input_spec(self):
    return collections.OrderedDict(
        x=tf.TensorSpec([None, 5], tf.float32),
        y=tf.TensorSpec([None, 1], tf.int32))

  @tf.function
  def predict_on_batch(self, x, training=True):
    del training
    return predict_on_batch(self._variables, x)

  @tf.function
  def forward_pass(self, batch, training=True):
    del training
    loss, predictions = dred_forward_pass(self._variables, batch)
    num_exmaples = tf.shape(batch['x'])[0]
    return tff.learning.models.BatchOutput(
        loss=loss, predictions=predictions, num_examples=num_exmaples)

  @tf.function
  def report_local_unfinalized_metrics(
      self) -> collections.OrderedDict[str, list[tf.Tensor]]:
    """creates an `ordereddict` of metric names to unfinalized values."""
    return get_local_unfinalized_metrics(self._variables)

  def metric_finalizers(
      self) -> collections.OrderedDict[str, Callable[[list[tf.Tensor]], tf.Tensor]]:
    """creates an `ordereddict` of metric names to finalizers."""
    return get_metric_finalizers()

  @tf.function
  def reset_metrics(self):
    """resets metrics variables to initial value."""
    for var in self.local_variables:
      var.assign(tf.zeros_like(var))
#print("hellous")

"""the abstract methods and properties defined by
`tff.learning.models.variablemodel` corresponds to the code snippets in the preceding section
that introduced the variables and defined the loss and statistics.

here are a few points worth highlighting:

*   all state that our model will use must be captured as tensorflow variables,
    as tff does not use python at runtime (because of this we can implement the code such that it can be deployed to raspi;
*   our model should describe what form of data it accepts (`input_spec`), as
    in general, tff is a strongly-typed environment and wants to determine type
    signatures for all components. declaring the format of our model's input is
    an essential part of it.
*
    logic (forward pass, metric calculations, etc.) as `tf.function`s,
    as this helps ensure the tensorflow can be serialized, and removes the need
    for explicit control dependencies.

the above is sufficient for evaluation and algorithms like federated sgd.
however, for federated averaging, we need to specify how the model should train
locally on each batch. we will specify a local optimizer when building the federated averaging algorithm.

### simulating federated training with the new model

with all the above in place, the remainder of the process looks like what we've
seen already - just replace the model constructor with the constructor of our
new model class, and use the two federated computations in the iterative process
you created to cycle through training rounds.
"""
print("hellothey")
#weight = greet_server.Sayhello,weights
in_weight = None
in_bias = None
def train(weight=None, bias=None):
  global in_weight,in_bias
  training_process = tff.learning.algorithms.build_weighted_fed_avg(
      dredmodel,
      client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
      server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

  if weight is None and bias is None:
      in_weight = weight
      in_bias = bias
      training_process = tff.learning.algorithms.build_weighted_fed_avg(
          dredmodel2,
          client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
          server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))
  else:
      training_process = tff.learning.algorithms.build_weighted_fed_avg(
          dredmodel,
          client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
          server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))

  train_state = training_process.initialize()
  #print("hellowe")

  for round_num in range(1, 3):# it should be 5 instead of 3
    result = training_process.next(train_state, federated_train_data)
    train_state = result.state
    metrics = result.metrics
    result
    print('round {:2d}, metrics={}'.format(round_num, metrics))


  # assuming train_state.global_model_weights contains the given output
  global_model_weights = train_state.global_model_weights

  # extracting the trainable weights
  trainable_weights_array = global_model_weights.trainable[0]
  trainable_bias_array = global_model_weights.trainable[1]

  # converting to numpy arrays if needed
  trainable_weights_array_np = np.array(trainable_weights_array)
  flattened_weights_array = [item for sublist in trainable_weights_array_np for item in sublist]
  trainable_bias_array_np = np.array(trainable_bias_array)

  #def trainable_weights_array_np():
  #  return np.array(trainable_weights_array)
  #def trainable_bias_array_np():
  #  return np.array(trainable_bias_array)

  print("trainable weights array:")
  print(trainable_weights_array_np)
  print("\ntrainable bias array:")
  print(trainable_bias_array_np)
  return flattened_weights_array,trainable_bias_array_np

"""*** grpc***"""
